<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>WaveNet: A Generative Model for Raw Audio</title><style>
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.highlight-default {
}
.highlight-gray {
	color: rgb(155,154,151);
}
.highlight-brown {
	color: rgb(100,71,58);
}
.highlight-orange {
	color: rgb(217,115,13);
}
.highlight-yellow {
	color: rgb(223,171,1);
}
.highlight-teal {
	color: rgb(15,123,108);
}
.highlight-blue {
	color: rgb(11,110,153);
}
.highlight-purple {
	color: rgb(105,64,165);
}
.highlight-pink {
	color: rgb(173,26,114);
}
.highlight-red {
	color: rgb(224,62,62);
}
.highlight-gray_background {
	background: rgb(235,236,237);
}
.highlight-brown_background {
	background: rgb(233,229,227);
}
.highlight-orange_background {
	background: rgb(250,235,221);
}
.highlight-yellow_background {
	background: rgb(251,243,219);
}
.highlight-teal_background {
	background: rgb(221,237,234);
}
.highlight-blue_background {
	background: rgb(221,235,241);
}
.highlight-purple_background {
	background: rgb(234,228,242);
}
.highlight-pink_background {
	background: rgb(244,223,235);
}
.highlight-red_background {
	background: rgb(251,228,228);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(55, 53, 47, 0.6);
	fill: rgba(55, 53, 47, 0.6);
}
.block-color-brown {
	color: rgb(100,71,58);
	fill: rgb(100,71,58);
}
.block-color-orange {
	color: rgb(217,115,13);
	fill: rgb(217,115,13);
}
.block-color-yellow {
	color: rgb(223,171,1);
	fill: rgb(223,171,1);
}
.block-color-teal {
	color: rgb(15,123,108);
	fill: rgb(15,123,108);
}
.block-color-blue {
	color: rgb(11,110,153);
	fill: rgb(11,110,153);
}
.block-color-purple {
	color: rgb(105,64,165);
	fill: rgb(105,64,165);
}
.block-color-pink {
	color: rgb(173,26,114);
	fill: rgb(173,26,114);
}
.block-color-red {
	color: rgb(224,62,62);
	fill: rgb(224,62,62);
}
.block-color-gray_background {
	background: rgb(235,236,237);
}
.block-color-brown_background {
	background: rgb(233,229,227);
}
.block-color-orange_background {
	background: rgb(250,235,221);
}
.block-color-yellow_background {
	background: rgb(251,243,219);
}
.block-color-teal_background {
	background: rgb(221,237,234);
}
.block-color-blue_background {
	background: rgb(221,235,241);
}
.block-color-purple_background {
	background: rgb(234,228,242);
}
.block-color-pink_background {
	background: rgb(244,223,235);
}
.block-color-red_background {
	background: rgb(251,228,228);
}
.select-value-color-default { background-color: rgba(206,205,202,0.5); }
.select-value-color-gray { background-color: rgba(155,154,151, 0.4); }
.select-value-color-brown { background-color: rgba(140,46,0,0.2); }
.select-value-color-orange { background-color: rgba(245,93,0,0.2); }
.select-value-color-yellow { background-color: rgba(233,168,0,0.2); }
.select-value-color-green { background-color: rgba(0,135,107,0.2); }
.select-value-color-blue { background-color: rgba(0,120,223,0.2); }
.select-value-color-purple { background-color: rgba(103,36,222,0.2); }
.select-value-color-pink { background-color: rgba(221,0,129,0.2); }
.select-value-color-red { background-color: rgba(255,0,26,0.2); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="56f350ef-f2f3-487c-8b0d-61691dacb354" class="page sans"><header><h1 class="page-title">WaveNet: A Generative Model for Raw Audio</h1><table class="properties"><tbody><tr class="property-row property-row-checkbox"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesCheckbox"><path d="M0,3 C0,1.34314 1.34326,0 3,0 L11,0 C12.6567,0 14,1.34314 14,3 L14,11 C14,12.6569 12.6567,14 11,14 L3,14 C1.34326,14 0,12.6569 0,11 L0,3 Z M3,1.5 C2.17139,1.5 1.5,2.17157 1.5,3 L1.5,11 C1.5,11.8284 2.17139,12.5 3,12.5 L11,12.5 C11.8286,12.5 12.5,11.8284 12.5,11 L12.5,3 C12.5,2.17157 11.8286,1.5 11,1.5 L3,1.5 Z M2.83252,6.8161 L3.39893,6.27399 L3.57617,6.10425 L3.92334,5.77216 L4.26904,6.10559 L4.44531,6.27582 L5.58398,7.37402 L9.28271,3.81073 L9.45996,3.64008 L9.80664,3.3056 L10.1538,3.63989 L10.3311,3.81067 L10.8936,4.35303 L11.0708,4.52399 L11.4434,4.88379 L11.0708,5.24353 L10.8936,5.41437 L6.1084,10.0291 L5.93115,10.2 L5.58398,10.5344 L5.23682,10.2 L5.05957,10.0292 L2.83057,7.87946 L2.65283,7.70801 L2.27832,7.34674 L2.6543,6.98694 L2.83252,6.8161 Z"></path></svg></span>구현여부</th><td><div class="checkbox checkbox-off"></div></td></tr><tr class="property-row property-row-select"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesSelect"><path d="M7,13 C10.31348,13 13,10.31371 13,7 C13,3.68629 10.31348,1 7,1 C3.68652,1 1,3.68629 1,7 C1,10.31371 3.68652,13 7,13 Z M3.75098,5.32278 C3.64893,5.19142 3.74268,5 3.90869,5 L10.09131,5 C10.25732,5 10.35107,5.19142 10.24902,5.32278 L7.15771,9.29703 C7.07764,9.39998 6.92236,9.39998 6.84229,9.29703 L3.75098,5.32278 Z"></path></svg></span>분류</th><td></td></tr><tr class="property-row property-row-select"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesSelect"><path d="M7,13 C10.31348,13 13,10.31371 13,7 C13,3.68629 10.31348,1 7,1 C3.68652,1 1,3.68629 1,7 C1,10.31371 3.68652,13 7,13 Z M3.75098,5.32278 C3.64893,5.19142 3.74268,5 3.90869,5 L10.09131,5 C10.25732,5 10.35107,5.19142 10.24902,5.32278 L7.15771,9.29703 C7.07764,9.39998 6.92236,9.39998 6.84229,9.29703 L3.75098,5.32278 Z"></path></svg></span>유형</th><td></td></tr><tr class="property-row property-row-file"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesFile"><path d="M5.94578,14 C4.62416,14 3.38248,13.4963 2.44892,12.585 C1.514641,11.6736 1,10.4639 1,9.17405 C1.00086108,7.88562 1.514641,6.67434 2.44892,5.76378 L7.45612,0.985988 C8.80142,-0.327216 11.1777,-0.332396 12.5354,0.992848 C13.9369,2.36163 13.9369,4.58722 12.5354,5.95418 L8.03046,10.2414 C7.16278,11.0877 5.73682,11.0894 4.86024,10.2345 C3.98394,9.37789 3.98394,7.98769 4.86024,7.1327 L6.60422,5.4317 L7.87576,6.67196 L6.13177,8.37297 C6.01668,8.48539 6.00003,8.61545 6.00003,8.68335 C6.00003,8.75083 6.01668,8.88103 6.13177,8.99429 C6.36197,9.21689 6.53749,9.21689 6.76768,8.99429 L11.2707,4.70622 C11.9645,4.03016 11.9645,2.91757 11.2638,2.23311 C10.5843,1.57007 9.40045,1.57007 8.72077,2.23311 L3.71342,7.0109 C3.12602,7.58406 2.79837,8.35435 2.79837,9.17405 C2.79837,9.99459 3.12602,10.7654 3.72045,11.3446 C4.90947,12.5062 6.98195,12.5062 8.17096,11.3446 L10.41911,9.15165 L11.6906,10.3919 L9.4425,12.585 C8.50808,13.4963 7.2664,14 5.94578,14 Z"></path></svg></span>자료</th><td></td></tr><tr class="property-row property-row-created_time"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesCreatedAt"><path d="M6.98643729,14.0000972 C5.19579566,14.0000972 3.40419152,13.3106896 2.04245843,11.9323606 C-0.681017475,9.21200555 -0.680780251,4.76029539 2.04293482,2.04012507 C4.76664406,-0.68004331 9.22427509,-0.68004331 11.9480135,2.04013479 C13.272481,3.36277455 14,5.1330091 14,6.99552762 C14,8.87640182 13.2721894,10.6285043 11.9480135,11.9509302 C10.5679344,13.3105924 8.77756503,14.0000972 6.98643729,14.0000972 Z M10.2705296,7.00913883 L10.2705296,8.46099754 L10.2705296,8.65543362 L10.076181,8.65543362 L8.6543739,8.65543362 L5.72059514,8.65543362 L5.52619796,8.65543362 L5.52619796,8.46099754 L5.52619796,5.52541044 L5.52619796,3.37946773 L5.52619796,3.18502193 L5.72059514,3.18502193 L7.17253164,3.18502193 L7.36692883,3.18502193 L7.36692883,3.37946773 L7.36692883,6.81467358 L10.076181,6.81467358 L10.2705296,6.81467358 L10.2705296,7.00913883 Z M12.1601539,6.99552762 C12.1601539,5.61697497 11.6190112,4.32597154 10.6393933,3.34769528 C8.63253764,1.34336744 5.35197452,1.34061603 3.34153136,3.33944106 C3.33868273,3.34219247 3.33607716,3.34494388 3.33322852,3.34769528 C1.32397148,5.35459953 1.32372842,8.63641682 3.33322852,10.6433794 C5.34295224,12.6504489 8.62968901,12.6504489 10.6393933,10.6433794 C11.6190112,9.66506426 12.1601539,8.37408027 12.1601539,6.99552762 Z"></path></svg></span>작성일시</th><td><time>@2021년 1월 12일 오후 9:06</time></td></tr></tbody></table></header><div class="page-body"><h1 id="5cc5a3c3-3f89-46a3-9499-a1a563337657" class="">Abstract</h1><h3 id="42c7ca20-62a0-49f6-a2fc-7e24b2c7fd93" class=""><strong>WaveNet</strong></h3><ul id="6a07b3f8-4b90-4770-a05f-fa072371f8cb" class="bulleted-list"><li>Raw audio waveform을 생성하는 DNN - Generative Model</li></ul><ul id="003fc600-f1e5-48e3-abed-b9317ada8955" class="bulleted-list"><li>특징: Fully probabilistic and Autoregressive<ul id="09e2a005-f675-4469-8a71-a099c93eb152" class="bulleted-list"><li>Fully probabilistic:  random variable과 probability distribution을 사용하여 modeling함.</li></ul><ul id="b9b9fcbb-520c-49f3-ad22-25ef22892c0b" class="bulleted-list"><li>Autoregressive: 변수의 과거 값들을 조합하여 관심있는 변수를 예측함.</li></ul></li></ul><p id="f0f56973-4220-4f8e-9721-12f74e826a93" class="">⇒ 현재시점 t에서의 audio sample의 predictive distribution을 이전 시점의 audio sample에 대한((Autoregressive)) 조건부 확률로 modeling함(Fully probabilistic).</p><h3 id="ffb0f298-6584-4ecb-940b-ff8b68e61348" class=""><strong>WaveNet의 쓰임</strong></h3><ul id="ba360fb3-784a-441a-b75c-ffc79c33337d" class="bulleted-list"><li>Text-to-Speech</li></ul><ul id="3b770e76-ecb1-4fb1-9175-651da1e50861" class="bulleted-list"><li>1개의 model로 여러 화자의 특징을 잡아낼 수 있고, conditioning을 통해 화자에서 화자로 바꿀 수도 있음.<p id="e3de00e2-1666-4fab-ab4c-ddaa17230c94" class="">(이전의 TTS 모델은 대부분 음성 데이터를 쪼개고 조합해서 생성하는 방식이었기 때문에 화자나 톤을 바꾸고자 할 때 마다 새로운 모델/데이터가 필요했음.)</p></li></ul><ul id="d97e920a-108e-4deb-a712-c5628deca8ae" class="bulleted-list"><li>음악 생성</li></ul><ul id="240915a1-a180-4ce8-8d9e-a6ebb31777b2" class="bulleted-list"><li>phoneme(음소) recognition</li></ul><p id="a31f39c6-9ead-4d44-81c7-784d6705d198" class="">
</p><h1 id="72525a40-0780-4382-b0ab-fd7d2592e33b" class="">1. Introduction</h1><h3 id="07d52ac1-dffc-4c21-9ce5-4d0c26140a92" class=""><strong>Neural Autoregressive Generative Model</strong></h3><ul id="1a7ad29d-5ba6-41ec-9a87-549f9363adc9" class="bulleted-list"><li>Generative Model의 일종. Autroregressive를 이용.</li></ul><ul id="0962cee0-c782-4de2-9282-5fc5d11d1217" class="bulleted-list"><li>이미지나 텍스트 등 복잡한 distribution을 modeling한 것.</li></ul><ul id="d739b4d7-7204-41b7-a684-4a1ef64db9d3" class="bulleted-list"><li>어떻게?<p id="bcebef58-faea-402a-aeec-bcf2332c717b" class="">: Neural Net을 사용하여 <span style="border-bottom:0.05em solid">픽셀/단어의 joint probability를 modeling</span> 했는데, 이때 <span style="border-bottom:0.05em solid">joint probability= (조건부 확률의 곱)</span> 임을 이용.</p></li></ul><ul id="95dcadca-3d86-43c7-88b7-56e88e28487f" class="bulleted-list"><li>이걸 음성(wideband raw audio waveform)에도 적용해보면 어떨까? ⇒ WaveNet의 탄생</li></ul><h3 id="df5773d3-aa95-4fda-80c6-8f2293c09711" class=""><strong>WaveNet</strong></h3><ul id="d2c37592-9278-492a-8869-3f5b6a904c6b" class="bulleted-list"><li>PixelCNN에 기반한 audio generative model</li></ul><ul id="9c7f0402-3fa3-4f2e-9742-12cda5c7eba7" class="bulleted-list"><li>음성의 특징인 long-range temporal dependency 문제를 해결하기 위해 <span style="border-bottom:0.05em solid">Dilated Causal Convolution</span>을 사용함</li></ul><ul id="57a81d7d-db1b-484e-a2b7-23b9f01cc7f8" class="bulleted-list"><li>성과<ul id="30c2464a-797c-41d7-9ac7-cfa305b21fc2" class="bulleted-list"><li>TTS 분야에서 자연스로운 raw speech signal을 생성할 수 있음.</li></ul><ul id="500cd09a-14c8-407a-8162-464f8272e5ea" class="bulleted-list"><li>하나의 모델로 여러 종류의 목소리를 생성할 수 있음.</li></ul><ul id="37441fa5-3f1a-43ee-ac3e-e4425926a622" class="bulleted-list"><li>speech recognition 잘하고, music같은 다른 modality에도 적용가능.</li></ul><p id="678c4c4a-6c15-4916-95f0-25a120182d6f" class="">
</p></li></ul><h1 id="754f2fd8-a823-4673-bbac-72618d4d911c" class="">2. WaveNet</h1><figure id="cc7bc522-253f-428d-a17e-7beb3283f65b" class="image"><a href="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled.png"><img style="width:773px" src="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled.png"/></a></figure><ul id="f09f1a04-4447-4595-bc81-6258ec8a480f" class="bulleted-list"><li>특정 timestep t에서의 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span>는 모든 이전 timestep에 대해 condition됨.(조건부 확률)</li></ul><ul id="b6f14239-e397-48be-8757-f2429d9eeef3" class="bulleted-list"><li>그리고 이 조건부 확률을 모두 곱하면, 특정 timestep t와 그것의 모든 과거시점<style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>X</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><mi>T</mi></msub><mo stretchy="false">}</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(X = \{x_1, x_2, ... , x_T\})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span>의 joint probability가 된다.</li></ul><figure id="c3f561f1-8b8a-48fa-88c7-07623b328a08" class="image"><a href="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/1.jpg"><img style="width:750px" src="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/1.jpg"/></a><figcaption>참고 1. joint probability를 조건부 확률의 곱으로 나타낼 수 있는 이유 (chain rule) </figcaption></figure><ul id="baff8585-1dbf-4345-93f3-354c6a905fa4" class="bulleted-list"><li>모델 전반 요약<ul id="da497f44-db83-40b7-962b-a80f7cd9e528" class="bulleted-list"><li>conditional probability distribution을 modeling하기 위해 convolution layer를 쌓음.</li></ul><ul id="413c230a-efe0-45ba-91b2-1c81328e8fc6" class="bulleted-list"><li>output과 input의 time dimensionality는 동일.</li></ul><ul id="c95c850f-21c4-404f-995d-c2d2dc9b1275" class="bulleted-list"><li>output: softmax를 거져서 나온 categorical distribution(다음 timestep의 값)</li></ul><ul id="3703b074-c68f-4ed6-8c18-2ba0ceddd664" class="bulleted-list"><li>optimization: loglikelihood를 최대화하는 방향으로 parameter를 update</li></ul></li></ul><p id="da7f221c-9f26-4da7-bf5c-a623d74e8783" class="">
</p><h2 id="a4ca5ce7-0a28-4ac8-8ba4-49b27a5fbc0d" class="">2.1. Dilated Causal Convolutions</h2><ul id="139c2111-9d2c-4101-9db7-0acbfed2318b" class="toggle"><li><details open=""><summary>Standard Convolution(참고용)</summary><figure id="77ad1329-6b0b-45c7-bc00-ef46a71fe7af" class="image"><a href="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%201.png"><img style="width:875px" src="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%201.png"/></a></figure></details></li></ul><h3 id="9a178a7c-98e0-42fa-8062-839630874e24" class="">Causal Convolution</h3><figure id="1f23bc4d-8cee-45e1-b1b1-ea455136f4db" class="image"><a href="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%202.png"><img style="width:648px" src="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%202.png"/></a></figure><ul id="8a268284-3eea-41f8-b4b6-305bd0c56909" class="bulleted-list"><li>timestep t의 output을 내기 위해 timestep t까지의 input들만을 고려하는 방식</li></ul><ul id="37224a1e-56ae-47d5-867e-79a5b66c5ec4" class="bulleted-list"><li>목적: CNN을 시계열 데이터에 사용할 때, <span style="border-bottom:0.05em solid">모델이 data의 순서를 violate하지 않도록</span> 하기 위함.<p id="a0403ce9-a8bb-481a-a4a7-1b5d36953c7e" class="">(future timestep이 들어가면 안됨.)</p></li></ul><ul id="01a5c02e-3ad1-43fe-aabd-cf9628d04056" class="bulleted-list"><li>과정<ul id="7ce17c41-fe8b-44ba-9c9b-09c712458097" class="bulleted-list"><li>Training에서는 각 timestep별로 pararell하게 연산 가능.(RNN과의 비교했을 때의 장점)</li></ul><ul id="50e525d3-7d19-4abf-b76d-df430a8f1684" class="bulleted-list"><li>Generation 단계에서는 sequential.</li></ul></li></ul><ul id="58296bcf-8a62-46f7-8344-97f5bd99d129" class="bulleted-list"><li>장점<p id="c61590c4-28a1-4f84-913f-5de1d906d97a" class="">: recurrent connection이 없기 때문에 빠르다 → 특히 data가 길 때 good.</p></li></ul><ul id="3ef29d6a-425d-4c78-b983-a41cd8d5c81b" class="bulleted-list"><li>단점<p id="7b17621b-4b5f-4c53-bcba-89e4f64c94b6" class="">: receptive를 늘리기 위해 많은 수의 레이어 또는 큰 필터를 필요로 한다.<div class="indented"><p id="89ba0c25-cac8-48e0-901c-c538ec20217b" class="">⇒ 이 문제를 해결한 것이 Dilated Convolution!</p></div></p></li></ul><h3 id="77388c9b-9e03-4e7b-83c0-76b4ca9bdd76" class="">Dilated Convolution</h3><p id="4585d7d7-14ea-4364-9675-2bc59a7496cd" class="">
</p><figure id="70e48f7f-9b27-473e-8087-f85d14783963" class="image"><a href="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%203.png"><img style="width:654px" src="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%203.png"/></a><figcaption>causal conv와 dilated conv가 합쳐진 형태, dilation이 각각 1, 2, 4, 8인 경우</figcaption></figure><ul id="21efc7d8-f36a-440f-b3f9-48532c82441f" class="bulleted-list"><li>특정 step(dilation)마다 input 값을 받는 것.</li></ul><ul id="ddba32b6-92c7-47bd-939b-5b14a82beb78" class="bulleted-list"><li>목적: 적은 수의 layer로 넓은 범위의 receptive field를 받는 것 → long term dependency를 capture할 수 있음. </li></ul><ul id="8920cd16-2145-4e22-8279-c36c9253787d" class="bulleted-list"><li>장점: 같은 효과를 내는 방법들 중 가장 효율적<p id="7a76118e-e588-477f-839a-53f4ad1370c9" class="">1) 적은 layer수로 large receptive field를 갖게 됨. 연산량 감소 (vs applying large layer)</p><p id="ddb7e463-6855-4bca-8872-33ea65b87dde" class="">2) <span style="border-bottom:0.05em solid">input shape이 그대로 유지</span>되어 나옴. 정보 유실 적음. (vs stride, pooling)</p><p id="7e500870-8675-47ae-b9b9-03d1c162f230" class="">3) non-linear 연산이 많이 들어가 model이 더 discriminative해짐.(vs applying large filter)</p><p id="18a30544-bac2-4a20-8140-8df06b106e3b" class="">
</p></li></ul><ul id="1a4bdb3e-e823-49e2-9787-847e99d8f9d4" class="bulleted-list"><li>WaveNet에서의 활용<ul id="455084e7-2ac7-4e08-bf0e-3f698a8f5cbe" class="bulleted-list"><li>512를 limit으로 잡은 뒤, dilation을 layer 마다 2배씩 늘리고, limit에 도달하면 다시 1부터 시작. <ul id="97383667-d9bc-42b2-9544-49b0d76c83b9" class="bulleted-list"><li>Intuition 1) Dilation을 2배씩 늘리면 receptive field도 2배씩 증가</li></ul><ul id="1ece71d7-85c4-420b-826c-6d64219bdb86" class="bulleted-list"><li>Intuition 2) block을 stacking하면 model capacity와 receptive field도 더더욱 증가</li></ul></li></ul></li></ul><h2 id="3648addc-076e-4e18-8c17-16ca33911e99" class="">2.2 Softmax Distributions</h2><h3 id="1f8dee65-434a-484f-a222-8ae3214a4274" class="">Modeling Conditional Distribution</h3><ul id="89cb11e7-708f-4a2d-872c-131567bc861a" class="bulleted-list"><li>Softmax Distribution으로 modeling(Classification task)<ul id="c70be0b3-5713-420a-bd79-301b806147c2" class="bulleted-list"><li>마지막에 softmax를거쳐서 categorical하게 나옴. (나올 수 있는 모든 값들의 확률을 1-D로 담아서 그 중 확률 제일 큰거 선택)</li></ul></li></ul><h3 id="ef415f3b-03a7-47ca-8002-a2c994e5a1da" class="">Quantization(양자화)</h3><ul id="b25f4074-f762-4817-97fe-5458ccfb03c2" class="bulleted-list"><li>무한대의 가지수를 갖는 음성 data의 값을 유한한 몇 가지 대표값으로 바꿔주는 것.<p id="cf63bb6c-b340-48e8-9fb3-1befd0c69293" class="">ex) 0~2 → 0,   2~4 →2,   4~6→4 ...</p></li></ul><ul id="662cf750-a170-4c04-8aa6-e2040ef41d2a" class="bulleted-list"><li>Raw audio가 16bit이기 때문에 만약 그대로 쓰게된다면 output으로 가능한 값이 2^16 = 65,536개이고, 이 말은 softmax에서 나타내는 class가 65,536개라는 뜻 → 너무 많다.</li></ul><ul id="4ee40d40-95ad-45c9-916f-becac3c27960" class="bulleted-list"><li>이를 해결하기 위해 양자화 도입, <span style="border-bottom:0.05em solid">input data에</span> mu-law compounding transformation을 사용하여 8-bit(256개)로 바꿔줌 → softmax를 통해 max 값을 정하면 얘를 다시 reconstriction해서 원래 raw audio 값(16bit)으로 바꿔줌.</li></ul><figure id="b22fb8e7-bc68-4832-b94b-e9b05cd94b60" class="image"><a href="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%204.png"><img style="width:281px" src="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%204.png"/></a><figcaption>mu-law compounding transformation 공식</figcaption></figure><ul id="e0fbc53f-7ac4-4fe6-9175-8db4728458ea" class="bulleted-list"><li>양자화에는 linear과 nonlinear 방식이 있는데 nonlinear 방식(mu-law compounding)이 더 좋았음</li></ul><h2 id="0bfe2119-95e9-44ab-981e-46f7d88fc169" class="">2.3 Gated Activation Units</h2><ul id="a86266c2-ba30-4fe3-acb6-045f6c2a7a07" class="bulleted-list"><li>gated PixelCNN에서 사용된 gated activation과 동일</li></ul><h3 id="55fba284-6bec-4340-8360-6d589016275e" class="">목적</h3><ul id="59764def-1aec-4030-a2b5-a5a141f56df1" class="bulleted-list"><li>Pixel CNN이 Pixel RNN에 비해 연산 속도는 빠르지만 성능이 더 낮다는 점을 보완하기 위해 고안됨.</li></ul><h3 id="fcf5d924-47d1-45aa-96ab-c03648fea357" class="">수식</h3><figure id="98008d7e-58e5-49b3-86da-f67f9702c140" class="image"><a href="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%205.png"><img style="width:312px" src="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%205.png"/></a><figcaption>                       gated activation 수식</figcaption></figure><ul id="e45ca270-bf6f-48d5-8453-a12c31f9a7a8" class="bulleted-list"><li>Filter 부분과 Gate 부분의 elementwise mutiplication이라고 볼 수 있음.</li></ul><ul id="5cb443a5-7102-43d3-a833-c0daed039ef8" class="bulleted-list"><li>Filter 부분<ul id="d86bc2eb-fa4a-4a7b-8af8-f0cfd2bfe8c8" class="bulleted-list"><li>일반적인 convolution 연산(dilated) 후 activation으로 tanh</li></ul><ul id="3dabc526-24d2-4a4b-afaa-7b024032cb8a" class="bulleted-list"><li>특정 layer에서 뽑아진 local feature를 의미.</li></ul></li></ul><ul id="0078a18b-9c3f-481c-a34d-ccedaf4acbed" class="bulleted-list"><li>Gate 부분<ul id="2c9b1455-4035-4f6f-829e-9bd484ae1822" class="bulleted-list"><li>일반적인 convolution 연산 후(dilated) activation으로 sigmoid(0-1사이 값)</li></ul><ul id="d8dacdfb-4e11-4afa-bc92-90a15eea9f0c" class="bulleted-list"><li>Intuition: Filter 부분의 값(정보)를 다음 layer에 얼마만큼 정해줄지를 결정함.</li></ul></li></ul><h3 id="9c26795f-edae-4fab-884f-5678bf71f9bd" class="">이게 왜 잘될까?</h3><p id="70dfd33b-e210-4eb9-8167-2b6fa3d5157a" class="">RNN이 가진 장점을 보완했기 때문이라 생각.</p><ul id="c5baaf58-204d-4b87-b285-42e63a37c383" class="bulleted-list"><li>RNN이 더 잘됐던 원인<p id="aa5c2b21-65cb-4737-8016-b55583d48f55" class="">1)  LSTM의 recurrent conntection으로 인해 모든 layer가 이전 pixel의 entire neighborhood에 접근하게 됨. 반면 ReLu를 쓴다면, neighborhood의 영역이 layer 깊이에 따라 linear하게 증가함.</p><p id="e84d9252-508d-4b1c-8316-d5c0679e9b4b" class="">(WaveNet에서는 해당이 안된다고 생각하는데.. 이해 확인하기)</p><p id="e6ee322a-2d00-4e40-8b17-a8146e1b5fc7" class="">2) multiplicative unit이 들어가 더 복잡한 interaction을 model하는데 도움을 주었을 것.</p></li></ul><ul id="6053b1e6-d7cd-41b2-8870-1b9608c89a4a" class="bulleted-list"><li>내 생각: Swish(Self-Gated Activation)의 형태와 비슷하다고 생각한다.<ul id="1008067a-e5ae-4709-86e0-dffffb00093b" class="bulleted-list"><li>Swish 식 : <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi><mo>⋅</mo><mi>σ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x) = x\cdot \sigma(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.44445em;vertical-align:0em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span></li></ul><ul id="24307854-5d61-4bb6-aaa2-55d43217d095" class="bulleted-list"><li>ReLU와 달리 negative value에서 미분 값이 0이 되지 않음.(Leaky ReLU 해결) + 그 외 장점</li></ul></li></ul><p id="fad94fd5-2e75-49a6-b288-219dee53cb74" class="">
</p><ul id="aa0352da-12f6-40aa-94f7-002b0e13381a" class="toggle"><li><details open=""><summary>gated pixelCNN 구조(참고)</summary><p id="04dded05-c069-4fb2-8044-6026767f6bf2" class="">
</p><figure id="43dd7137-6504-48e5-b180-8fbbec03731b" class="image"><a href="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%206.png"><img style="width:834px" src="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%206.png"/></a></figure></details></li></ul><ul id="2b936dc0-64a0-4aff-8a90-287a336d2b6f" class="toggle"><li><details open=""><summary>Swish 그래프</summary><p id="bafd4a94-222e-4556-b4d9-9ceec93fca0f" class="">
</p><figure id="ce55514e-0ead-4148-b245-2fac6c9811f1" class="image"><a href="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%207.png"><img style="width:801px" src="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%207.png"/></a></figure></details></li></ul><h2 id="4bd2861e-b4b3-417e-8704-68f524540cd4" class="">2.4 Residual and Skip Connections</h2><h3 id="4b2e6a50-bb3e-4917-9354-18dbfcc47481" class="">목적</h3><ul id="dc812838-2c4e-4f42-8390-402ee9870710" class="bulleted-list"><li>Convergence 속도를 높이고 더 깊은 network의 사용을 용이하게 하기 위함.<p id="5f603810-48b9-485b-8110-1f6b73944204" class="">(residual connection이 gradient flow에 효과적)</p></li></ul><ul id="dccbcbef-c16b-4b70-9852-53a78021b47c" class="bulleted-list"><li>1X1 convolution 연산: 연산량을 줄이고, shape을 맞추는 용도<figure id="07eb1c8f-41f5-4eb3-84d3-8fc5d28bc82c" class="image"><a href="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%208.png"><img style="width:594px" src="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%208.png"/></a></figure><figure id="dd770047-a226-4e67-8a85-d5acbffc5f8e" class="image"><a href="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%209.png"><img style="width:594px" src="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%209.png"/></a><figcaption>Heiga Zen, <a href="https://www.youtube.com/watch?v=nsrSrYtKkT8">https://www.youtube.com/watch?v=nsrSrYtKkT8</a></figcaption></figure></li></ul><h2 id="77226dab-8e90-462d-b824-3f2c891c5e48" class="">2.5 Conditional WaveNets</h2><h3 id="4e30d0ae-fd8a-4a3f-8205-857dd8bcec57" class="">목적</h3><ul id="c5aa388b-c5ed-4bf1-819b-b7f8cd7d4669" class="bulleted-list"><li>원하는 characteristic을 가진 audio를 generate할 수 있게 하기 위함.</li></ul><h3 id="f4e1b909-ccf2-43de-8295-14f28a4dafd8" class="">수식</h3><figure id="0c7399dd-1570-44cf-9af0-229fcfb5e8fd" class="image"><a href="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%2010.png"><img style="width:330px" src="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%2010.png"/></a></figure><ul id="065256c6-b221-4aa0-80cf-21d3cadd553c" class="bulleted-list"><li>additional input h를 넣어 conditional distribution을 model하도록 함.</li></ul><h3 id="59081b79-48e9-4c47-9c5c-06beb68f0d9e" class="">방식</h3><ul id="961acd3d-c217-4ed2-85bb-5f1cc41666af" class="bulleted-list"><li>Global Conditioning<ul id="21c17ef0-a310-4809-8972-021afc2f7723" class="bulleted-list"><li>h가 전체 timestep에서 고정됨 ex) userID(TTS)</li></ul></li></ul><figure id="8a9d0cc1-fd2d-44aa-9965-a156b0c81871" class="image"><a href="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%2011.png"><img style="width:458px" src="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%2011.png"/></a><figcaption>                                      V는 학습되는 파라미터</figcaption></figure><ul id="e085ef97-4ad4-4e11-9d89-6816b3b4cfee" class="bulleted-list"><li>Local Conditioning<ul id="465014ba-36c7-4b11-84eb-35634e3c02d3" class="bulleted-list"><li>h가 전체 timestep에서 변함, timeseries로 들어감<style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(h_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span> ex) linguistic feature</li></ul><ul id="cd94a981-40b2-46a9-9e1e-7010b8130cab" class="bulleted-list"><li>먼저 input h의 timestep이 audio의 timestep과 같아지게 하기 위해 transposed convolution network를 적용한다. <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(y = h(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span><p id="caaa8cc3-fe70-42b2-aa2b-87d84b4a9b6d" class="">→transposed conv를 하는 이유는 보통 h의 sampling frequency가 더 낮기 때문.</p></li></ul></li></ul><figure id="c88508d4-95e4-4c32-aade-c975b01d036c" class="image"><a href="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%2012.png"><img style="width:500px" src="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%2012.png"/></a></figure><ul id="00871870-d32b-44bf-806d-68bd4987058a" class="toggle"><li><details open=""><summary>참고: Transposed Convolution</summary><p id="2c16c889-541e-4f85-9f49-23b2373d7fa4" class="">
</p><figure id="3c1ca5a0-e722-4125-a516-c0dea087500e" class="image"><a href="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%2013.png"><img style="width:885px" src="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%2013.png"/></a></figure></details></li></ul><h2 id="9062cba8-9c45-467f-a370-f34b8370ebac" class="">2.6 Context Stacks</h2><ul id="1e6fd48f-235e-4fd8-9de8-7b2a5144a231" class="bulleted-list"><li>Receptive field size를 늘릴 수 있는 또 다른 방법.</li></ul><h3 id="e45db8d0-004f-4890-b87c-7d312497cffb" class="">방법</h3><ul id="f54eb9a1-9ad1-4dc6-813d-06afdd3145da" class="bulleted-list"><li>두 개(또는 N개)의 WaveNet을 stack하는 것. <p id="6f499406-7e26-4778-8458-50b905c9c959" class="">→ receptive field가 작은 shorter-range WaveNet(large)와 receptive field가 넓은 longer-range WaveNet(small)</p></li></ul><ul id="1ab24729-f024-450a-b232-8d6654567b7b" class="bulleted-list"><li>Shoter-range WaveNet이 메인 모델이고, 여기에 local conditioning을 하자! 이때 추가로 들어올 input <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span>는 longer range WaveNet의 output(softmax안함).</li></ul><figure id="6c24f23b-34ad-4a6a-b4bb-ddad8e3c4c8a" class="image"><a href="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/context_stack.jpg"><img style="width:750px" src="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/context_stack.jpg"/></a></figure><h3 id="359d9ee3-5ac4-4d82-9d86-299fcffc4b4f" class="">효과</h3><ul id="09c93df6-fffc-4c74-a50a-09211bf68052" class="bulleted-list"><li>Longer-range WaveNet의 output이 input으로 들어오기 때문에 Receptive field는 longer range WaveNet의 receptive field만큼이 됨.</li></ul><ul id="3bb18ba3-a470-47f8-a813-d5792cfe0d33" class="bulleted-list"><li>Longer-range WaveNet에서는 pooling layer가 사용될 수 있기 때문에 같은 receptive field를 가진 single WaveNet과 비교했을 때 연산량을 줄일 수 있음.</li></ul><p id="d83c9dc5-e7c1-4cad-a807-821c88054768" class="">
</p><p id="a7307305-aea1-4839-83e5-0eb853cee057" class="">*두 WaveNet이 모두 전체 data를 보는 것인지, 아니면 나눠서 일부만 보는 것인지 햇갈리는데 답변해주시면 감사하겠습니다!</p><h1 id="91015e97-ca41-45b3-8535-2f8fb72d7702" class="">3. Experiments</h1><h2 id="c6a116b6-3781-4f03-b66a-2d9e01089791" class="">3.1. Multi-Speaker Speech Generation</h2><ul id="3258add7-b3b8-4d82-bf5d-710e34ce7ee4" class="bulleted-list"><li>Dataset: English multi-speaker corpus from CSTR voice cloning toolkit</li></ul><ul id="57b99780-3713-474f-b9fa-9636f3b17e23" class="bulleted-list"><li>위에서 나온 <span style="border-bottom:0.05em solid">Global conditioning</span>을 활용하여 User ID를 새로운 input h로 넣어 학습 진행(one-hot vector 형태).</li></ul><ul id="caa5529b-b6a4-44e3-9d67-ce35b714f1eb" class="bulleted-list"><li>User ID 를 함께 넣어주면 이후 generation 단계에서 해당 화자의 목소리로 output을 냄.</li></ul><ul id="48e66615-3544-4720-a134-32b0b5e930ab" class="bulleted-list"><li>training 단계에서 해당 화자의 목소리만 가지고 training하는 것보다 다른 화자들의 목소리도 함께 사용해서 training 했을 때 성능이 더 좋았음. → wavenet의 internal representation이 multiple speaker들 사이에서 share된다.</li></ul><ul id="81d78e91-6bf4-4a58-97c0-7672d9e4f7e6" class="bulleted-list"><li>voice 뿐 아니라 다른 음성적 특징(음질, 숨소리 등)도 함께 잡아냄.</li></ul><h2 id="09a547c2-3832-432f-9836-fabd297129ea" class="">3.2 Text-to-Speech</h2><ul id="e58b1e97-da44-4c90-95d3-28de88746917" class="bulleted-list"><li>Dataset: North American English and Mandarine Chinese dataset</li></ul><ul id="f208e4b7-5b54-4b9c-9ad1-38b77f3e0bd2" class="bulleted-list"><li>Local conditioning을 활용하여 각각 linguistic feature와 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">F_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span>값(=기본주파수)에 condition시켜 학습 진행.</li></ul><ul id="c8827dff-6e5c-4a4f-9647-a0dab9f96644" class="bulleted-list"><li>기존 모델들보다 성능 좋았고, WaveNet끼리 비교했을 때는 linguistic feature에만 condition되었을 때보다 linguistic feature와 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">F_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span>값 모두에 condition되었을 때의 성능이 더 좋았음.</li></ul><h2 id="e91ae882-9a74-46ef-ae5e-3d35b68173e7" class="">3.3 Music</h2><ul id="2b2f5bdd-8ea7-4533-83e2-b1fa0fb3f1ce" class="bulleted-list"><li>Dataset: MagnaTagATune, YouTube piano dataset</li></ul><ul id="bb620f7b-c03e-4fb9-816f-60e1b30aa335" class="bulleted-list"><li>Tag를 Global Conditioning을 통해 추가 input으로 넣었으면(binary vector이용), tag에 맞는 음악이 생성됨.</li></ul><h2 id="fe05b6ac-c060-40c0-81f5-29615705e5de" class="">3.4 Speech Recognition</h2><ul id="2eecdfae-2e30-4427-8329-44d879f5216f" class="bulleted-list"><li>Dataset: TIMIT</li></ul><ul id="f9203325-3cfb-42d3-b613-337b5467dad4" class="bulleted-list"><li>Generation이 아닌 discrimination task(맞추는거)</li></ul><ul id="2ab93686-0a7b-4a81-bf1b-77cc5627669f" class="bulleted-list"><li>Network 구조 살짝 변형<ul id="bf0503be-421b-4645-85b2-336446bf6aff" class="bulleted-list"><li>dilated convolution 사이에 [mean pooling layer→ non-causal convolution] 넣음</li></ul><ul id="76d58cad-794e-466f-99ef-f698c22d515a" class="bulleted-list"><li>Loss : 다음 sample predict용, classification용</li></ul></li></ul><ul id="f5782c93-d6ae-4c6d-a2a8-3051e2033c87" class="bulleted-list"><li>TIMIT data에서 best score냄.</li></ul><h1 id="69c83c4a-1a4c-4e1e-ac6f-b8ffb464c6d2" class="">4. Conclusion</h1><h3 id="3e5029a4-78f8-4eb0-bdaf-89cc2012dac1" class="">WaveNet</h3><ul id="09de6034-9f32-4add-a4ab-9a654e057b9e" class="bulleted-list"><li>Deep generative model for audio data(waveform)</li></ul><ul id="a14e9b9d-3b6d-4d0e-9813-3472784ad37d" class="bulleted-list"><li>특징: Autoregressive, dilated convolution</li></ul><ul id="ac6fb031-8aaf-4cb9-ac01-569b2a43e694" class="bulleted-list"><li>다른 input에 global 또는 local하게 condition됨으로서 원하는 특징을 가진 output을 만들어낼 수 있음.</li></ul><ul id="f927f0d5-9f42-4591-bb21-649e7e2615f6" class="bulleted-list"><li>TTS, music audio modeling, speech recognition task에서 good.</li></ul><h1 id="08ce3b9a-2227-4de8-8d65-e167f97880dc" class="">Appendix</h1><h2 id="ff17ac73-c8bf-4fb4-88bd-782fdcbc54e7" class="">A. Text-to-Speech Background</h2><ul id="8786b5f1-9b22-43a1-a316-a19132d7cb9e" class="bulleted-list"><li>Goal : text를 speech로 render하는 것(sequence to sequence mapping problem)</li></ul><ul id="094f90e5-e49f-4494-b63c-46cf0f6039df" class="bulleted-list"><li>사람은 이를 어떻게 하는가? → 이를 비스무리하게 컴퓨터에 적용해보자</li></ul><ul id="468dc970-7ce8-4880-81b1-af1def8e44eb" class="bulleted-list"><li>TTS pipeline<p id="26847937-3a5a-4f23-9569-3fa7fd87ea73" class="">1) Text analysis part: input = word sequence, output = phoneme sequence</p><p id="cb4331dd-0ef1-4c40-a169-4857ed0225a5" class="">2) Speech synthesis part: input = phoneme sequence, output = speech waveform</p></li></ul><ul id="16c9d181-8985-4c4a-88cd-bc81956c9ab1" class="bulleted-list"><li>Speech synthesis(pipeline의 두번째 부분)에서의 2가지 Main approach<p id="f9b2fc55-f6c7-4621-ad4e-0ce212c7547e" class="">1) non-parametric, concatenative approach</p><p id="27c043aa-fd06-4924-85fd-ba2954b9a912" class="">2) statistical parametric approach</p></li></ul><ul id="486e4dd9-536b-4122-beb0-516481ca471d" class="bulleted-list"><li>Statistical parametric approach의 과정 </li></ul><figure id="1cb07724-14ce-4b8a-b4c5-3a42cc01007e" class="image"><a href="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%2014.png"><img style="width:510px" src="WaveNet%20A%20Generative%20Model%20for%20Raw%20Audio%20cc7bc522253f428da17e7beb3283f65b/Untitled%2014.png"/></a></figure><p id="93a4fd2d-41d0-4686-b3cd-a52bd99b1198" class="">등등등</p><p id="b580fa1a-4217-44a6-b0d8-0eee2e292454" class="">
</p><h2 id="cbdc9453-f18a-40ae-bd85-138482cdc2a3" class="">B. Details of TTS Experiment</h2><ul id="a79d62e4-1643-43c2-87d8-16330fda8b91" class="bulleted-list"><li>sampling과 실험에 사용된 data setting</li></ul><ul id="c95ff567-66b3-429e-a842-aadaf6e45d82" class="bulleted-list"><li>사용된 linguistic feature</li></ul><ul id="019e8d33-ab51-401d-9776-5b33c3a2d0a2" class="bulleted-list"><li>subjective listening test가 어떤 방식으로 이루어졌는지</li></ul><p id="dc80f601-4095-46c9-89cc-3f502db4e755" class="">
</p><p id="b1698dd0-8290-4ba3-a9a6-afc401a0776a" class="">
</p><p id="425d2fc8-e56a-4a20-9536-1f3f8d1257d0" class="">
</p><p id="13d43378-6cbd-425d-882a-0dace8e4ed33" class="">
</p><h3 id="9b74b7dd-ea3f-47aa-98d0-d8431dfa8a01" class="">출처)</h3><p id="2e7c9dc3-5dfb-4e29-8bcf-82aa73d257f2" class="">mu-law compounding <a href="https://m.blog.naver.com/PostView.nhn?blogId=sbkim24&amp;logNo=10084099777&amp;proxyReferer=https:%2F%2Fwww.google.com%2F">https://m.blog.naver.com/PostView.nhn?blogId=sbkim24&amp;logNo=10084099777&amp;proxyReferer=https:%2F%2Fwww.google.com%2F</a></p><p id="0f857111-5edd-47f7-bd8b-a2c3b22678a4" class="">Gated PixelCNN <a href="https://arxiv.org/pdf/1606.05328.pdf">https://arxiv.org/pdf/1606.05328.pdf</a></p><p id="6049d4f9-3cdf-4809-9975-718b3d1f0c63" class="">Swish <a href="https://medium.com/techspace-usict/swish-a-self-gated-activation-function-3b7e551dacb5">https://medium.com/techspace-usict/swish-a-self-gated-activation-function-3b7e551dacb5</a></p><p id="806eb6e0-5644-485e-9e6d-c0677ebfb9cf" class="">Generative Model-Based Text-to-Speech Synthesis <a href="https://www.youtube.com/watch?v=nsrSrYtKkT8">https://www.youtube.com/watch?v=nsrSrYtKkT8</a> </p><p id="9ce1baba-4e2c-4197-a69a-587ded9ff248" class="">[논문리뷰] WaveNet <a href="https://joungheekim.github.io/2020/09/17/paper-review/">https://joungheekim.github.io/2020/09/17/paper-review/</a></p><p id="ad7dddf8-462b-4f54-9ce3-2e7b3cf7d33b" class="">모두의 연구소 김성일님 발제영상 <a href="https://www.youtube.com/watch?v=GyQnex_DK2k">https://www.youtube.com/watch?v=GyQnex_DK2k</a></p><p id="0346bfe3-e077-4036-ab63-77838e093417" class="">Generative Model-Based Text-to-Speech Synthesis <a href="https://www.youtube.com/watch?v=nsrSrYtKkT8">https://www.youtube.com/watch?v=nsrSrYtKkT8</a></p><p id="d5d4f284-8d23-45db-b38c-9e52a967db95" class="">GIthub-issue: context stack부분 관련 <a href="https://github.com/ibab/tensorflow-wavenet/issues/164">https://github.com/ibab/tensorflow-wavenet/issues/164</a></p><p id="a0c7da6a-9443-4e56-9090-2316da5df5ca" class="">long-term dependency 관련 <a href="https://brunch.co.kr/@chris-song/9#:~:text=%EC%9E%A5%EA%B8%B0%20%EC%9D%98%EC%A1%B4%EC%84%B1">https://brunch.co.kr/@chris-song/9#:~:text=장기 의존성</a>(Long%2DTerm%20Dependency,%EB%8F%84%EC%9B%80%EC%9D%84%20%EC%A4%84%20%EC%88%98%20%EC%9E%88%EC%A3%A0.</p><p id="6f131597-2c90-473a-af94-39279b0250e2" class="">Transposed Convolution <a href="https://dataplay.tistory.com/29">https://dataplay.tistory.com/29</a></p><p id="39f94268-95c3-4434-8e16-7e748f3181b9" class="">
</p></div></article></body></html>